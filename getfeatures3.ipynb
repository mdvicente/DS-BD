{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re, string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "fname = 'tweets.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = [line.rstrip() for line in open(fname)]\n",
    "file = open(\"newfile.txt\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def checkTopic (topic, text):\n",
    "    if (set(topic) & set(text)):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def bag_of_words(words):\n",
    "    if len(words) > 0:\n",
    "        return dict([(word, True) for word in words])\n",
    "\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"contributors\": null, \n",
      "    \"coordinates\": null, \n",
      "    \"created_at\": \"Sun Jun 12 18:07:18 +0000 2016\", \n",
      "    \"entities\": {\n",
      "        \"hashtags\": [], \n",
      "        \"symbols\": [], \n",
      "        \"urls\": [], \n",
      "        \"user_mentions\": []\n",
      "    }, \n",
      "    \"favorite_count\": 0, \n",
      "    \"favorited\": false, \n",
      "    \"filter_level\": \"low\", \n",
      "    \"geo\": null, \n",
      "    \"id\": 742055693281288200, \n",
      "    \"id_str\": \"742055693281288193\", \n",
      "    \"in_reply_to_screen_name\": null, \n",
      "    \"in_reply_to_status_id\": null, \n",
      "    \"in_reply_to_status_id_str\": null, \n",
      "    \"in_reply_to_user_id\": null, \n",
      "    \"in_reply_to_user_id_str\": null, \n",
      "    \"is_quote_status\": false, \n",
      "    \"lang\": \"en\", \n",
      "    \"place\": null, \n",
      "    \"retweet_count\": 0, \n",
      "    \"retweeted\": false, \n",
      "    \"source\": \"<a href=\\\"http://twitter.com/download/iphone\\\" rel=\\\"nofollow\\\">Twitter for iPhone</a>\", \n",
      "    \"text\": \"Thread of my fav mutuals \\ud83d\\udc96\", \n",
      "    \"timestamp_ms\": \"1465754838663\", \n",
      "    \"truncated\": false, \n",
      "    \"user\": {\n",
      "        \"contributors_enabled\": false, \n",
      "        \"created_at\": \"Wed Nov 20 19:22:25 +0000 2013\", \n",
      "        \"default_profile\": false, \n",
      "        \"default_profile_image\": false, \n",
      "        \"description\": \"just needed time to, to find my own.\", \n",
      "        \"favourites_count\": 21669, \n",
      "        \"follow_request_sent\": null, \n",
      "        \"followers_count\": 11247, \n",
      "        \"following\": null, \n",
      "        \"friends_count\": 558, \n",
      "        \"geo_enabled\": true, \n",
      "        \"id\": 2205462907, \n",
      "        \"id_str\": \"2205462907\", \n",
      "        \"is_translator\": false, \n",
      "        \"lang\": \"pt\", \n",
      "        \"listed_count\": 52, \n",
      "        \"location\": \"3562.4 miles away to Pickering\", \n",
      "        \"name\": \"cus misses shawn :(\", \n",
      "        \"notifications\": null, \n",
      "        \"profile_background_color\": \"C0DEED\", \n",
      "        \"profile_background_image_url\": \"http://pbs.twimg.com/profile_background_images/504280325652873216/PnT0iM6p.png\", \n",
      "        \"profile_background_image_url_https\": \"https://pbs.twimg.com/profile_background_images/504280325652873216/PnT0iM6p.png\", \n",
      "        \"profile_background_tile\": true, \n",
      "        \"profile_banner_url\": \"https://pbs.twimg.com/profile_banners/2205462907/1464795622\", \n",
      "        \"profile_image_url\": \"http://pbs.twimg.com/profile_images/739525583139266560/sC50_G4k_normal.jpg\", \n",
      "        \"profile_image_url_https\": \"https://pbs.twimg.com/profile_images/739525583139266560/sC50_G4k_normal.jpg\", \n",
      "        \"profile_link_color\": \"89C9FA\", \n",
      "        \"profile_sidebar_border_color\": \"000000\", \n",
      "        \"profile_sidebar_fill_color\": \"DDEEF6\", \n",
      "        \"profile_text_color\": \"333333\", \n",
      "        \"profile_use_background_image\": true, \n",
      "        \"protected\": false, \n",
      "        \"screen_name\": \"soulmendes\", \n",
      "        \"statuses_count\": 199028, \n",
      "        \"time_zone\": \"Amsterdam\", \n",
      "        \"url\": \"http://ask.fm/shawnsoulmate\", \n",
      "        \"utc_offset\": 7200, \n",
      "        \"verified\": false\n",
      "    }\n",
      "}\n",
      "nf_timestamp 1465754838663\n",
      "nf_user soulmendes\n"
     ]
    }
   ],
   "source": [
    "json_input = lines[4]\n",
    "decoded = json.loads(json_input)\n",
    "    \n",
    "# pretty printing of json-formatted string\n",
    "print json.dumps(decoded, sort_keys=True, indent=4)\n",
    "\n",
    "nf_timestamp = decoded['timestamp_ms']\n",
    "print \"nf_timestamp\", nf_timestamp\n",
    "\n",
    "nf_user = decoded['user']['screen_name']\n",
    "print \"nf_user\", nf_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['terrorism']\n",
      "12 9 1 ** perezhilton heartbreaking that i teach kids day one love respect everyone this still terrorism towards gays\n",
      "109 79 2 ** rt realdonaldtrump is president obama going finally mention words radical islamic terrorism if immediately r\n",
      "138 100 3 ** rt nero this terrorism it islamism it extremism this islam\n",
      "282 189 4 ** rt realdonaldtrump is president obama going finally mention words radical islamic terrorism if immediately r\n",
      "612 432 5 ** rt andreagrimes i fear officials calling terrorism\n",
      "830 586 6 ** rt thegopreport looks like jeff sessions right attacks we need trump lead us amp strong terrorism http\n",
      "919 655 7 ** potus realdonaldtrump hillaryclinton radical islamic terrorism hate wow blinded ideology made us unsafe fact\n",
      "7 710 999 0\n",
      "156 7 95\n",
      "28 1 11\n",
      "[u'perezhilton heartbreaking that i teach kids day one love respect everyone this still terrorism towards gays', u'rt realdonaldtrump is president obama going finally mention words radical islamic terrorism if immediately r', u'rt nero this terrorism it islamism it extremism this islam', u'rt realdonaldtrump is president obama going finally mention words radical islamic terrorism if immediately r', u'rt andreagrimes i fear officials calling terrorism', u'rt thegopreport looks like jeff sessions right attacks we need trump lead us amp strong terrorism http', u'potus realdonaldtrump hillaryclinton radical islamic terrorism hate wow blinded ideology made us unsafe fact']\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "exception = 0\n",
    "topic = [\"terrorism\"]\n",
    "print topic\n",
    "corpus = []\n",
    "label = []\n",
    "count = 0\n",
    "counts = []\n",
    "words = []\n",
    "sentences = []\n",
    "\n",
    "for index in range(len(lines)):\n",
    "    json_input = lines[index]\n",
    "    decoded = json.loads(json_input)\n",
    "    \n",
    "    try:\n",
    "\n",
    "        nf_lang = decoded['user']['lang']\n",
    "        #print \"nf_lang\", nf_lang\n",
    "    \n",
    "        if nf_lang == 'en':\n",
    "            #print \"JSON parsing example: \", decoded['hashtags']\n",
    "            # cal treure entities and stop words\n",
    "            nf_tweet_text = decoded['text']\n",
    "            #print \"nf_tweet_text\", nf_tweet_text\n",
    "            nf_RT = (decoded['retweet_count']) > 0\n",
    "            #print \"nf_RT\", nf_RT\n",
    "            if nf_RT:\n",
    "                nf_rt_text = decoded['retweeted_status']['text']\n",
    "                nf_tweet_text = nf_rt_text\n",
    "            #print \"nf_tweet_text :: \", nf_tweet_text\n",
    "\n",
    "            # not alphabetical characters\n",
    "            nf_tweet_text = re.sub(r\"[^a-zA-Z]\", \" \", nf_tweet_text)\n",
    "            \n",
    "            s = nf_tweet_text.split()          \n",
    "            #print(s)\n",
    " \n",
    "            # filtered = [word for words in s if word not in stopwords.words('english')]\n",
    "            nf_tweet_filtered_text = [i for i in s if i not in stop]\n",
    "            #print \"nf_tweet_filtered_text :: \", nf_tweet_filtered_text\n",
    "            nf_tweet_filtered_text = [element.lower() for element in nf_tweet_filtered_text]\n",
    "            \n",
    "            # check the topic\n",
    "            aux = \" \".join( nf_tweet_filtered_text )\n",
    "            corpus.append(aux)\n",
    "            count = count + 1\n",
    "            nf_positiu = checkTopic(topic, nf_tweet_filtered_text)\n",
    "            if nf_positiu:\n",
    "                total = total + 1\n",
    "                label.append(1)\n",
    "                sentences.append(aux)\n",
    "                print index, count, total, \"**\", aux\n",
    "            else:\n",
    "                label.append(0)\n",
    "            counts.append(len(nf_tweet_text)) \n",
    "            words.append(len(nf_tweet_filtered_text))\n",
    "\n",
    "               \n",
    "            nf_words = []\n",
    "        \n",
    "            nf_count_hashtags = len(decoded['entities']['hashtags'])\n",
    "            #print \"nf_count_hastags\", nf_count_hashtags\n",
    "            nf_pos_first_hashtag = 0\n",
    "            nf_tweet_hashtags = []\n",
    "            if nf_count_hashtags > 0:\n",
    "                nf_pos_first_hashtag = decoded['entities']['hashtags'][0]['indices'][0]\n",
    "                #print \"nf_pos_first_hashtag\", nf_pos_first_hashtag\n",
    "                for i in range(nf_count_hashtags):\n",
    "                    nf_tweet_hashtags.append(decoded['entities']['hashtags'][i]['text'])\n",
    "                #print \"nf_tweet_hashtags\", nf_tweet_hashtags\n",
    "                nf_tweet_hashtags = [element.lower() for element in nf_tweet_hashtags]\n",
    "                nf_words.append(nf_tweet_hashtags)\n",
    "        \n",
    "            nf_count_urls = len(decoded['entities']['urls'])\n",
    "            #print \"nf_count_urls\", nf_count_urls\n",
    "            nf_pos_first_url = 0\n",
    "            nf_tweet_urls = []\n",
    "            if nf_count_urls > 0:\n",
    "                nf_pos_first_url = decoded['entities']['urls'][0]['indices'][0]\n",
    "                #print \"nf_pos_first_url\", nf_pos_first_url\n",
    "                for i in range (nf_count_urls):\n",
    "                    nf_tweet_urls.append(decoded['entities']['urls'][i]['url'])\n",
    "                #print nf_tweet_urls\n",
    "                #nf_tweet_urls = [element.lower() for element in nf_tweet_urls]\n",
    "                #nf_words.append(nf_tweet_urls)\n",
    "        \n",
    "            nf_count_mentions = len(decoded['entities']['user_mentions'])\n",
    "            #print \"nf_count_mentions\", nf_count_mentions\n",
    "            nf_pos_first_mention = 0\n",
    "            nf_tweet_mentions = []\n",
    "            if nf_count_mentions > 0:\n",
    "                nf_pos_first_mention = decoded['entities']['user_mentions'][0]['indices'][0]\n",
    "                #print \"nf_pos_first_mention\", nf_pos_first_mention    \n",
    "                for i in range(nf_count_mentions):\n",
    "                    nf_tweet_mentions.append(decoded['entities']['user_mentions'][i]['screen_name']) \n",
    "                #print \"nf_tweet_mentions\", nf_tweet_mentions\n",
    "                nf_tweet_mentions = [element.lower() for element in nf_tweet_mentions]\n",
    "                nf_words.append(nf_tweet_mentions) \n",
    "\n",
    "            num_list = [0]\n",
    "            if nf_pos_first_hashtag > 0:\n",
    "                num_list.append(nf_pos_first_hashtag)\n",
    "            if nf_pos_first_url > 0:\n",
    "                num_list.append(nf_pos_first_url)\n",
    "            if nf_pos_first_mention > 0:\n",
    "                num_list.append(nf_pos_first_mention)\n",
    "            nf_pos_first_entity = min(num_list)\n",
    "            #print \"nf_pos_first_entity\", nf_pos_first_entity\n",
    "\n",
    "            nf_user = decoded['user']['screen_name']\n",
    "            #print \"nf_user\", nf_user\n",
    "\n",
    "            nf_timestamp = decoded['timestamp_ms']\n",
    "            #print \"nf_timestamp\", nf_timestamp\n",
    "            \n",
    "            #file.write(nf_tweet_filtered_text, nf_positiu, nf_count_hashtags, nf_pos_first_hashtag)\n",
    "\n",
    "\n",
    "    except (KeyError):\n",
    "        print \"JSON format error\", KeyError, index\n",
    "        #print json.dumps(decoded, sort_keys=True, indent=4)\n",
    "        exception = exception + 1\n",
    "    except (ValueError):\n",
    "        print \"JSON format error\", ValueError, index\n",
    "        #print json.dumps(decoded, sort_keys=True, indent=4)\n",
    "        exception = exception + 1\n",
    "    except (TypeError):\n",
    "        print \"JSON format error\", TypeError, index\n",
    "        #print json.dumps(decoded, sort_keys=True, indent=4)\n",
    "        exception = exception + 1\n",
    "        \n",
    "\n",
    "print total, count, index, exception\n",
    "print max(counts), min(counts), sum(counts)/len(counts)\n",
    "print max(words), min(words), sum(words)/len(words)\n",
    "#print corpus\n",
    "print sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vectorizer = CountVectorizer(min_df=1, stop_words='english')\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7x49 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 72 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = vectorizer.fit_transform(sentences)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'right': 38, u'love': 26, u'perezhilton': 32, u'jeff': 21, u'radical': 35, u'unsafe': 46, u'realdonaldtrump': 36, u'officials': 31, u'looks': 25, u'nero': 29, u'immediately': 17, u'need': 28, u'respect': 37, u'fear': 8, u'hate': 12, u'blinded': 3, u'islam': 18, u'rt': 39, u'thegopreport': 44, u'lead': 23, u'wow': 48, u'finally': 9, u'going': 11, u'islamism': 20, u'terrorism': 43, u'hillaryclinton': 14, u'http': 15, u'sessions': 40, u'extremism': 6, u'andreagrimes': 1, u'islamic': 19, u'heartbreaking': 13, u'words': 47, u'president': 34, u'teach': 42, u'strong': 41, u'day': 5, u'obama': 30, u'kids': 22, u'like': 24, u'amp': 0, u'mention': 27, u'attacks': 2, u'calling': 4, u'ideology': 16, u'trump': 45, u'gays': 10, u'potus': 33, u'fact': 7}\n",
      "[u'amp', u'andreagrimes', u'attacks', u'blinded', u'calling', u'day', u'extremism', u'fact', u'fear', u'finally', u'gays', u'going', u'hate', u'heartbreaking', u'hillaryclinton', u'http', u'ideology', u'immediately', u'islam', u'islamic', u'islamism', u'jeff', u'kids', u'lead', u'like', u'looks', u'love', u'mention', u'need', u'nero', u'obama', u'officials', u'perezhilton', u'potus', u'president', u'radical', u'realdonaldtrump', u'respect', u'right', u'rt', u'sessions', u'strong', u'teach', u'terrorism', u'thegopreport', u'trump', u'unsafe', u'words', u'wow']\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "print (vectorizer.vocabulary_)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print vocab\n",
    "print len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0\n",
      "  1 0 0 0 0 1 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 1\n",
      "  0 0 1 0 0 0 1 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      "  0 0 1 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 1\n",
      "  0 0 1 0 0 0 1 0 0 0 1 0]\n",
      " [0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      "  0 0 1 0 0 0 1 0 0 0 0 0]\n",
      " [1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0\n",
      "  0 1 1 1 1 0 1 1 1 0 0 0]\n",
      " [0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1\n",
      "  0 0 0 0 0 0 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.    0.    0.    0.    0.    0.35  0.    0.    0.    0.    0.35  0.    0.\n",
      "   0.35  0.    0.    0.    0.    0.    0.    0.    0.    0.35  0.    0.    0.\n",
      "   0.35  0.    0.    0.    0.    0.    0.35  0.    0.    0.    0.    0.35\n",
      "   0.    0.    0.    0.    0.35  0.15  0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.32  0.    0.32\n",
      "   0.    0.    0.    0.    0.    0.32  0.    0.27  0.    0.    0.    0.    0.\n",
      "   0.    0.    0.32  0.    0.    0.32  0.    0.    0.    0.32  0.27  0.27\n",
      "   0.    0.    0.21  0.    0.    0.    0.16  0.    0.    0.    0.32  0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.47  0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.47  0.    0.47  0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.47  0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.26  0.    0.    0.    0.2   0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.32  0.    0.32\n",
      "   0.    0.    0.    0.    0.    0.32  0.    0.27  0.    0.    0.    0.    0.\n",
      "   0.    0.    0.32  0.    0.    0.32  0.    0.    0.    0.32  0.27  0.27\n",
      "   0.    0.    0.21  0.    0.    0.    0.16  0.    0.    0.    0.32  0.  ]\n",
      " [ 0.    0.47  0.    0.    0.47  0.    0.    0.    0.47  0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.47  0.    0.    0.    0.    0.    0.    0.\n",
      "   0.26  0.    0.    0.    0.2   0.    0.    0.    0.    0.  ]\n",
      " [ 0.27  0.    0.27  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.27  0.    0.    0.    0.    0.    0.27  0.    0.27  0.27\n",
      "   0.27  0.    0.    0.27  0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.27  0.15  0.27  0.27  0.    0.11  0.27  0.27  0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.32  0.    0.    0.    0.32  0.    0.    0.    0.\n",
      "   0.32  0.    0.32  0.    0.32  0.    0.    0.23  0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.32  0.    0.23  0.23\n",
      "   0.    0.    0.    0.    0.    0.    0.13  0.    0.    0.32  0.    0.32]]\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "np.set_printoptions(precision=2)\n",
    "print tfidf.fit_transform(vectorizer.fit_transform(sentences)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 amp\n",
      "2 andreagrimes\n",
      "1 attacks\n",
      "1 blinded\n",
      "4 calling\n",
      "1 day\n",
      "1 extremism\n",
      "1 fact\n",
      "1 fear\n",
      "2 finally\n",
      "6 gays\n",
      "2 going\n",
      "1 hate\n",
      "2 heartbreaking\n",
      "1 hillaryclinton\n",
      "1 http\n",
      "1 ideology\n",
      "1 immediately\n",
      "1 islam\n",
      "1 islamic\n",
      "1 islamism\n",
      "8 jeff\n",
      "2 kids\n",
      "1 lead\n",
      "3 like\n",
      "1 looks\n",
      "1 love\n",
      "1 mention\n",
      "4 need\n",
      "1 nero\n",
      "1 obama\n",
      "3 officials\n",
      "1 perezhilton\n",
      "1 potus\n",
      "2 president\n",
      "1 radical\n",
      "1 realdonaldtrump\n",
      "1 respect\n",
      "1 right\n",
      "1 rt\n",
      "1 sessions\n",
      "2 strong\n",
      "4 teach\n",
      "3 terrorism\n",
      "1 thegopreport\n",
      "1 trump\n",
      "1 unsafe\n",
      "1 words\n",
      "1 wow\n"
     ]
    }
   ],
   "source": [
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = train_data_features.toarray()\n",
    "\n",
    "# Sum up the counts of each vocabulary word\n",
    "dist = np.sum(train_data_features, axis=0)\n",
    "\n",
    "# For each, print the vocabulary word and the number of times it \n",
    "# appears in the training set\n",
    "# the word topic should be the most present\n",
    "for tag, count in zip(vocab, dist):\n",
    "    print count, tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the random forest...\n",
      "[0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0]\n",
      "710\n",
      "710\n"
     ]
    }
   ],
   "source": [
    "print \"Training the random forest...\"\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize a Random Forest classifier with 100 trees\n",
    "forest = RandomForestClassifier(n_estimators = 100) \n",
    "\n",
    "# Fit the forest to the training set, using the bag of words as \n",
    "# features and the sentiment labels as the response variable\n",
    "#\n",
    "# This may take a few minutes to run\n",
    "arraylabel = np.asarray(label)\n",
    "print arraylabel\n",
    "print len(arraylabel)\n",
    "print len(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forest = forest.fit( train_data_features, np.array(label))\n",
    "#predicted= model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perezhilton heartbreaking that i teach kids day one love respect everyone this still terrorism towards gays\n",
      "[u'perezhilton heartbreaking that i teach kids day one love respect everyone this still terrorism towards gays', u'rt realdonaldtrump is president obama going finally mention words radical islamic terrorism if immediately r', u'rt nero this terrorism it islamism it extremism this islam', u'rt realdonaldtrump is president obama going finally mention words radical islamic terrorism if immediately r', u'rt andreagrimes i fear officials calling terrorism', u'rt thegopreport looks like jeff sessions right attacks we need trump lead us amp strong terrorism http', u'potus realdonaldtrump hillaryclinton radical islamic terrorism hate wow blinded ideology made us unsafe fact']\n",
      "Training model...\n",
      "Word2Vec(vocab=24, size=300, alpha=0.025)\n",
      "fi\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u'm'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import gensim, logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "test = sentences[0]\n",
    "print test\n",
    "usersentences = sentences[1:]\n",
    "print sentences\n",
    "\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300   # Word vector dimensionality                      \n",
    "min_word_count = 1   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print \"Training model...\"\n",
    "model = word2vec.Word2Vec(usersentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "print model\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)\n",
    "print \"fi\"\n",
    "#model.accuracy('/tmp/questions-words.txt')\n",
    "model.doesnt_match(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigram_transformer = gensim.models.Phrases(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrases<212 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n"
     ]
    }
   ],
   "source": [
    "print bigram_transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=0, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "\n",
    "## create a w2v learner \n",
    "basemodel = Word2Vec(\n",
    "    workers=multiprocessing.cpu_count(), # use your cores\n",
    "    iter=3, # iter = sweeps of SGD through the data; more is better\n",
    "    hs=1, negative=0 # we only have scoring for the hierarchical softmax setup\n",
    "    )\n",
    "print(basemodel)\n",
    "\n",
    "basemodel.build_vocab(sentences)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
